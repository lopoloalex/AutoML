{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab7f38b5",
   "metadata": {},
   "source": [
    "<a id=\"tpo\"></a>\n",
    "\n",
    "### TPOT(Tree-Based Pipeline Optimization Tool)\n",
    "\n",
    "[TPOT](http://epistasislab.github.io/tpot/) es una herramienta de optimización de pipelines basada en árboles que utiliza algoritmos genéticos para optimizar las pipelines de aprendizaje automático. TPOT está construido sobre scikit-learn y utiliza sus propios métodos de regresión y clasificación. TPOT explora miles de posibles posibilidades y selecciona la que mejor se ajusta a los datos.\n",
    "\n",
    "TPOT no puede procesar automáticamente entradas de lenguaje natural. Además, tampoco es capaz de procesar cadenas categóricas, que deben ser codificadas con números enteros antes de ser pasadas como datos.\n",
    "\n",
    "\n",
    "![](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537396029/output_2_0_d7uh0v.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8339d70e",
   "metadata": {},
   "source": [
    "## Instalación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237c213",
   "metadata": {},
   "source": [
    "La instalación es bastante sencilla ya que se encuentra disponible a traves de pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc88f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tpot\n",
      "  Downloading TPOT-0.11.7-py3-none-any.whl (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 4.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting xgboost>=1.1.0\n",
      "  Downloading xgboost-1.4.0-py3-none-manylinux2010_x86_64.whl (166.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 166.7 MB 74.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.36.1 in /opt/conda/lib/python3.8/site-packages (from tpot) (4.60.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /opt/conda/lib/python3.8/site-packages (from tpot) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.16.3 in /opt/conda/lib/python3.8/site-packages (from tpot) (1.20.2)\n",
      "Collecting deap>=1.2\n",
      "  Downloading deap-1.3.1-cp38-cp38-manylinux2010_x86_64.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 43.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.24.2 in /opt/conda/lib/python3.8/site-packages (from tpot) (1.2.3)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /opt/conda/lib/python3.8/site-packages (from tpot) (1.6.2)\n",
      "Collecting update-checker>=0.16\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Collecting stopit>=1.1.1\n",
      "  Downloading stopit-1.1.2.tar.gz (18 kB)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /opt/conda/lib/python3.8/site-packages (from tpot) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.24.2->tpot) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.24.2->tpot) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->tpot) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.22.0->tpot) (2.1.0)\n",
      "Requirement already satisfied: requests>=2.3.0 in /opt/conda/lib/python3.8/site-packages (from update-checker>=0.16->tpot) (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.26.4)\n",
      "Building wheels for collected packages: stopit\n",
      "  Building wheel for stopit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for stopit: filename=stopit-1.1.2-py3-none-any.whl size=11955 sha256=0e53c6caa56e24368953ade0746b8ce46d148b431eacbf13e452666babe9a536\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a8/bb/8f/6b9328d23c2dcedbfeb8498b9f650d55d463089e3b8fc0bfb2\n",
      "Successfully built stopit\n",
      "Installing collected packages: xgboost, update-checker, stopit, deap, tpot\n",
      "Successfully installed deap-1.3.1 stopit-1.1.2 tpot-0.11.7 update-checker-0.18.0 xgboost-1.4.0\n"
     ]
    }
   ],
   "source": [
    "#conda install numpy scipy scikit-learn pandas joblib pytorch\n",
    "#pip install deap update_checker tqdm stopit xgboost\n",
    "!pip install tpot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6c55b",
   "metadata": {},
   "source": [
    "## Pruebas\n",
    "Como en los casos anteriores se va a realizar pruebas con el dataset de Titanic, ejemplos parecido a este se puede encontrar en la página oficial de [TPOT](http://epistasislab.github.io/tpot/examples/)\n",
    "\n",
    "Como se ha comentado, TPOT no acepta valores no numéricos como entrada, por lo que hay que realizar una serie de transformaciones al dataset antes de poder utilizarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fb2d050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os        \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split        \n",
    "df = pd.read_csv(\"./titanic/train.csv\") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb9a70",
   "metadata": {},
   "source": [
    "Si analizamos los campos que no pertenecen a este tipo de formato vemos que en el caso de Sex y Embarked se podría realizar un preprocesado previo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6191f78e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of levels in category 'Name': \b 891.00 \n",
      "Number of levels in category 'Sex': \b 2.00 \n",
      "Number of levels in category 'Ticket': \b 681.00 \n",
      "Number of levels in category 'Cabin': \b 148.00 \n",
      "Number of levels in category 'Embarked': \b 4.00 \n"
     ]
    }
   ],
   "source": [
    "for cat in ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']:\n",
    "    print(\"Number of levels in category '{0}': \\b {1:2.2f} \".format(cat, df[cat].unique().size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc421ca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levels for catgeory 'Sex': ['male' 'female']\n",
      "Levels for catgeory 'Embarked': ['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "for cat in ['Sex', 'Embarked']:\n",
    "    print(\"Levels for catgeory '{0}': {1}\".format(cat, df[cat].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b23a6",
   "metadata": {},
   "source": [
    "Modificamos manualmente estos valores a valores numéricos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7faa847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex'] = df['Sex'].map({'male':0,'female':1})\n",
    "df['Embarked'] = df['Embarked'].map({'S':0,'C':1,'Q':2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ea87a4",
   "metadata": {},
   "source": [
    "El primer paso es renombrar el feature que se usará como target a class.\n",
    "Los valores nan, son simplemente remplazados por el valor -999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c323435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Survived': 'class'}, inplace=True)\n",
    "df = df.fillna(-999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d0378",
   "metadata": {},
   "source": [
    "Dado que Name y Ticket tienen varios posibles valores, los eliminamos de nuestro análisis por simplicidad. Para Cabin, codificamos los niveles como dígitos utilizando el MultiLabelBinarizer de Scikit-learn y los tratamos como nuevas características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46fa338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "CabinTrans = mlb.fit_transform([{str(val)} for val in df['Cabin'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb14f223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CabinTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cadfbeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.drop(['Name','Ticket','Cabin','class'], axis=1)\n",
    "df_new = np.hstack((df_new.values,CabinTrans))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59dd8f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(df_new).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e35d1",
   "metadata": {},
   "source": [
    "Por último, almacenamos la feature class, que necesitamos predecir, en una variable separada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "842edf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_class = df['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9324031",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(668, 223)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_indices, validation_indices = training_indices, testing_indices = train_test_split(df.index, stratify = titanic_class, train_size=0.75, test_size=0.25)\n",
    "training_indices.size, validation_indices.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65fd8b",
   "metadata": {},
   "source": [
    "\n",
    "El package TPOTClassifier tiene una gran variedad de parámetros. Pero los más notables son:\n",
    "\n",
    "- generations: Número de iteraciones para el proceso de optimización de la pipeline de ejecución. El valor por defecto es 100.\n",
    "\n",
    "- population_size: Número de individuos por generación. El valor por defecto es 100.\n",
    "\n",
    "- offspring_size: Número de descendientes a producir en cada generación de programación. El valor por defecto es 100.\n",
    "\n",
    "- mutation_rate: Tasa de mutación para el algoritmo de programación en el rango [0,0, 1,0]. Este parámetro le dice al algoritmo cuántos cambios aleatorios debe de aplicar en cada generación. Por defecto es 0.9\n",
    "\n",
    "- crossover_rate: Tasa de cruce para cada generación en el rango [0,0, 1,0]. Este parámetro indica cuántas tuberías debe de generar en cada generación.\n",
    "\n",
    "- scoring: Función utilizada para evaluar la calidad de una pipeline dada para el problema de clasificación como exactitud, average_precision, roc_auc, recall, etc. El valor por defecto es accuracy.\n",
    "\n",
    "- cv: Estrategia de validación cruzada utilizada al evaluar las tuberías. El valor predeterminado es 5.\n",
    "\n",
    "- random_state: La semilla del generador de números pseudoaleatorios utilizado en TPOT. \n",
    "\n",
    "Como nuestro objetivo es sólo ilustrar el uso de TPOT, hemos fijado el tiempo máximo de optimización en 2 minutos (max_time_mins=2). En un portátil estándar con 4GB de RAM, se tarda aproximadamente 5 minutos en ejecutarse por generación. Por cada generación añadida, debería tardar 5 minutos más. Por lo tanto, para el valor por defecto de 100, el tiempo total de ejecución podría ser aproximadamente de unas 8 horas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e19cade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190dab52158e4cbda88119a42e2a050a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/40 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.8053192683200538\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.8069128043990574\n",
      "\n",
      "2.00 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: RandomForestClassifier(GaussianNB(input_matrix), bootstrap=True, criterion=gini, max_features=1.0, min_samples_leaf=6, min_samples_split=4, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(max_eval_time_mins=0.04, max_time_mins=2, population_size=40,\n",
       "               verbosity=2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from tpot import TPOTRegressor\n",
    "\n",
    "tpot = TPOTClassifier(verbosity=2, max_time_mins=2, max_eval_time_mins=0.04, population_size=40)\n",
    "tpot.fit(df_new[training_indices], titanic_class[training_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e8991de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.820627802690583"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.score(df_new[validation_indices], df.loc[validation_indices, 'class'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2180880",
   "metadata": {},
   "source": [
    "TPOT, por lo general, lleva una gran cantidad de tiempo en ejecutarse. Con la configuración predeterminada de TPOT (100 generaciones con 100 tamaños de población), TPOT evaluará 10.000 configuraciones diferentes antes de terminar. Se trata de 10.000 configuraciones de modelos con una validación cruzada de 10, lo que significa que aproximadamente 100.000 modelos se ajustan y evalúan para los datos suministrados. Es un procedimiento que lleva mucho tiempo, incluso para modelos más sencillos como los árboles de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48240a0",
   "metadata": {},
   "source": [
    "En el siguiente ejemplo, se ejecutan hasta 5 generaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fe95e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/600 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.817383009763214\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.817383009763214\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.817383009763214\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.817383009763214\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.817383009763214\n",
      "\n",
      "Best pipeline: GradientBoostingClassifier(input_matrix, learning_rate=0.1, max_depth=4, max_features=0.5, min_samples_leaf=12, min_samples_split=19, n_estimators=100, subsample=1.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(generations=5, verbosity=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=5, verbosity=2)\n",
    "tpot.fit(df_new[training_indices], titanic_class[training_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98a6b2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.820627802690583"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.score(df_new[validation_indices], df.loc[validation_indices, 'class'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9039a545",
   "metadata": {},
   "source": [
    "Una de las funcionales que distinguen a TPOT, es la capacidad de exportar el código Python correspondiente a la pipeline óptima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f594eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.export('pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba7e20e",
   "metadata": {},
   "source": [
    "## Ventajas:\n",
    "\n",
    "Fácil de usar y de instalar\n",
    "Entranamiento en paralelo con Dask\n",
    "Soporta redes neuronales como una característica experimental \n",
    "\n",
    "\n",
    "## Contras:\n",
    "\n",
    "TPOT puede tardar mucho tiempo en terminar su búsqueda. Ejecutar el TPOT no solo ajusta un modelo a un conjunto de datos. Utiliza múltiples algoritmos de aprendizaje automático (random forests, geométricos, SVMs, etc.) en una cadena con numerosos pasos de preprocesamiento (imputación de valores perdidos, escalado, PCA, selección de características, etc.), los hiperparámetros para todos los modelos y pasos de preprocesamiento, así como múltiples formas de ensamblar o apilar los algoritmos dentro de la cadena. Por ello, suele tardar mucho tiempo en ejecutarse y no es factible para grandes conjuntos de datos.\n",
    "\n",
    "TPOT puede recomendar diferentes modelos para el mismo conjunto de datos. Si se utiliza un conjunto de datos razonablemente complejo o se ejecuta el TPOT durante un corto período de tiempo, realizar diferentes ejecuciones de TPOT pueden dar lugar a distintas recomendaciones. Cuando dos ejecuciones de TPOT recomiendan diferentes pipelines, esto significa que no convergieron debido a la falta de tiempo o que múltiples pipelines funcionan más o menos igual en el conjunto de datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
